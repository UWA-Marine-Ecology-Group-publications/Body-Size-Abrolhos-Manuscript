rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
## Load Libraries ----
# To connect to GlobalArchive
install.packages("devtools")
install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
# To connect to GitHub
install.packages("RCurl")
install.packages("R.utils")
# To tidy data
install.packages("plyr")
install.packages("dplyr")
install.packages("tidyr")
install.packages("purrr")
install.packages("readr")
install.packages("stringr")
# to connect to googlesheets
install.packages("googlesheets")
install.packages("tidyverse")
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
# To connect to GitHub
library(RCurl)
library(R.utils)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
# to connect to googlesheets
library(googlesheets)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"bodysize"
## Set your working directory ----
working.dir<-dirname(rstudioapi::getActiveDocumentContext()$path) # to directory of current file - or type your own
#working.dir<-setwd("C:/Users/00104541/OneDrive - University of Wollongong/Tim Body Length")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Staging",sep="/")
download.dir<-paste(working.dir,"EM Export",sep="/")
tidy.dir<-paste(working.dir,"Tidy data",sep="/")
setwd(working.dir)
## Create EM Export, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "EM Export"))
dir.create(file.path(working.dir, "Staging"))
dir.create(file.path(working.dir, "Tidy data"))
# For csv file ----
metadata <-ga.list.files("_Metadata.csv")%>% # list all files ending in "_Metadata.csv"
purrr::map_df(~ga.read.files_em.csv(.))%>% # combine into dataframe
dplyr::select(campaignid,sample,latitude,longitude,date,time,location,status,site,depth,observer,successful.count,successful.length,comment)%>% # This line ONLY keep the 15 columns listed. Remove or turn this line off to keep all columns (Turn off with a # at the front).
glimpse()
# For csv file ----
metadata <-ga.list.files("_Metadata.csv")%>% # list all files ending in "_Metadata.csv"
purrr::map_df(~ga.read.files_em.csv(.))%>% # combine into dataframe
dplyr::select(campaignid,sample,latitude,longitude,date,time,location,status,site,depth,observer,successful.count,successful.length,comment)%>% # This line ONLY keep the 15 columns listed. Remove or turn this line off to keep all columns (Turn off with a # at the front).
glimpse()
setwd("C:/Users/User/OneDrive - University of Wollongong/Tim Body Length")
# For csv file ----
metadata <-ga.list.files("_Metadata.csv")%>% # list all files ending in "_Metadata.csv"
purrr::map_df(~ga.read.files_em.csv(.))%>% # combine into dataframe
dplyr::select(campaignid,sample,latitude,longitude,date,time,location,status,site,depth,observer,successful.count,successful.length,comment)%>% # This line ONLY keep the 15 columns listed. Remove or turn this line off to keep all columns (Turn off with a # at the front).
glimpse()
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
library(GlobalArchive)
uninstall_package("fansi")
uninstall_packages("fansi")
uninstall.packages("fansi")
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
remove.packages("fansi", lib="C:/Program Files/R/R-3.6.3/library")
install.packages(c("backports", "BH", "boot", "brio", "broman", "broom", "cli", "cowplot", "cpp11", "crayon", "crosstalk", "data.table", "DBI", "dbplyr", "deldir", "Deriv", "diffobj", "doBy", "dotCall64", "downlit", "DT", "emmeans", "expm", "fastmap", "foghorn", "GGally", "ggplot2", "ggrepel", "ggspatial", "ggthemes", "git2r", "gmp", "HH", "Hmisc", "hms", "htmltools", "htmlwidgets", "httpuv", "hunspell", "insight", "isoband", "jsonlite", "knitr", "lava", "lifecycle", "lme4", "lubridate", "Matrix", "matrixStats", "MCMCpack", "memoise", "mime", "mlbench", "multcomp", "pbkrtest", "pixmap", "pkgbuild", "pROC", "processx", "promises", "ps", "quantreg", "R.devices", "ragg", "rappdirs", "Rcpp", "RcppArmadillo", "RcppEigen", "recipes", "renv", "reprex", "rgdal", "rgl", "rmarkdown", "Rmpfr", "rstatix", "rstudioapi", "sandwich", "scales", "sf", "shiny", "sn", "sp", "spam", "SparseM", "spData", "spdep", "spelling", "SQUAREM", "statmod", "stringi", "sys", "systemfonts", "testthat", "textshaping", "tibble", "tinytex", "units", "vcd", "waldo", "whisker", "xfun"))
install.packages(c("coda", "DBI", "deldir", "dotCall64", "e1071", "expm", "fields", "matrixStats", "MCMCpack", "mnormt", "mvtnorm", "pixmap", "quantreg", "raster", "rgdal", "rgeos", "rgl", "sf", "sn", "spam", "SparseM", "spData", "spdep", "units"), lib="C:/Users/User/Documents/R/win-library/3.6")
install.packages(c("backports", "BH", "boot", "brio", "broman", "broom", "cli", "cowplot", "cpp11", "crayon", "crosstalk", "data.table", "DBI", "dbplyr", "deldir", "Deriv", "diffobj", "doBy", "dotCall64", "downlit", "DT", "emmeans", "expm", "fastmap", "foghorn", "GGally", "ggplot2", "ggrepel", "ggspatial", "ggthemes", "git2r", "gmp", "HH", "Hmisc", "hms", "htmltools", "htmlwidgets", "httpuv", "hunspell", "insight", "isoband", "jsonlite", "knitr", "lava", "lifecycle", "lme4", "lubridate", "Matrix", "matrixStats", "MCMCpack", "memoise", "mime", "mlbench", "multcomp", "pbkrtest", "pixmap", "pkgbuild", "pROC", "processx", "promises", "ps", "quantreg", "R.devices", "ragg", "rappdirs", "Rcpp", "RcppArmadillo", "RcppEigen", "recipes", "renv", "reprex", "rgdal", "rgl", "rmarkdown", "Rmpfr", "rstatix", "rstudioapi", "sandwich", "scales", "sf", "shiny", "sn", "sp", "spam", "SparseM", "spData", "spdep", "spelling", "SQUAREM", "statmod", "stringi", "sys", "systemfonts", "testthat", "textshaping", "tibble", "tinytex", "units", "vcd", "waldo", "whisker", "xfun"))
install.packages(c("coda", "DBI", "deldir", "dotCall64", "e1071", "expm", "fields", "matrixStats", "MCMCpack", "mnormt", "mvtnorm", "pixmap", "quantreg", "raster", "rgdal", "rgeos", "rgl", "sf", "sn", "spam", "SparseM", "spData", "spdep", "units"), lib="C:/Users/User/Documents/R/win-library/3.6")
rm(list=ls()) # Clear memory
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
## Load Libraries ----
# To connect to GlobalArchive
install.packages("devtools")
install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
# To connect to GitHub
install.packages("RCurl")
install.packages("R.utils")
# To tidy data
install.packages("plyr")
install.packages("dplyr")
install.packages("tidyr")
install.packages("purrr")
install.packages("readr")
install.packages("stringr")
# to connect to googlesheets
install.packages("googlesheets")
install.packages("tidyverse")
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
library(GlobalArchive)
# To connect to GitHub
library(RCurl)
library(R.utils)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
# to connect to googlesheets
library(googlesheets)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"bodysize"
## Set your working directory ----
working.dir<-dirname(rstudioapi::getActiveDocumentContext()$path) # to directory of current file - or type your own
#working.dir<-setwd("C:/Users/00104541/OneDrive - University of Wollongong/Tim Body Length")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Staging",sep="/")
download.dir<-paste(working.dir,"EM Export",sep="/")
tidy.dir<-paste(working.dir,"Tidy data",sep="/")
setwd(working.dir)
# For csv file ----
metadata <-ga.list.files("_Metadata.csv")%>% # list all files ending in "_Metadata.csv"
purrr::map_df(~ga.read.files_em.csv(.))%>% # combine into dataframe
dplyr::select(campaignid,sample,latitude,longitude,date,time,location,status,site,depth,observer,successful.count,successful.length,comment)%>% # This line ONLY keep the 15 columns listed. Remove or turn this line off to keep all columns (Turn off with a # at the front).
glimpse()
unique(metadata$campaignid) # check the number of campaigns in metadata, and the campaign name
setwd(staging.dir)
write.csv(metadata,paste(study,"metadata.csv",sep="_"),row.names = FALSE)
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn()%>%
dplyr::inner_join(metadata)%>%
dplyr::filter(successful.count=="Yes")%>%
dplyr::filter(maxn>0)%>%
glimpse()
# Save MaxN file ----
setwd(staging.dir)
rlang::last_error()
library(dplyr, lib.loc = "C:/Program Files/R/R-3.6.3/library")
detach("package:dplyr", unload = TRUE)
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn()%>%
dplyr::inner_join(metadata)%>%
dplyr::filter(successful.count=="Yes")%>%
dplyr::filter(maxn>0)%>%
glimpse()
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn()%>%
dplyr::inner_join(metadata)%>%
dplyr::filter(successful.count=="Yes")%>%
dplyr::filter(maxn>0)
library(dplyr, lib.loc = "C:/Program Files/R/R-3.6.3/library")
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
install_github("UWAMEGFisheries/GlobalArchive", force = TRUE) #to check for updates
library(GlobalArchive)
# To connect to GitHub
library(RCurl)
library(R.utils)
# To tidy data
library(plyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
# to connect to googlesheets
library(googlesheets)
library(dplyr, lib.loc = "C:/Program Files/R/R-3.6.3/library")
remove.packages("dplyr")
library(dplyr)
detach("package:tidyr", unload = TRUE)
detach("package:googlesheets", unload = TRUE)
library(dplyr)
library(dplyr, lib.loc = "C:/Program Files/R/R-3.6.3/library")
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
library(GlobalArchive)
# To connect to GitHub
library(RCurl)
library(R.utils)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
# to connect to googlesheets
library(googlesheets)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"bodysize"
## Set your working directory ----
working.dir<-dirname(rstudioapi::getActiveDocumentContext()$path) # to directory of current file - or type your own
#working.dir<-setwd("C:/Users/00104541/OneDrive - University of Wollongong/Tim Body Length")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Staging",sep="/")
download.dir<-paste(working.dir,"EM Export",sep="/")
tidy.dir<-paste(working.dir,"Tidy data",sep="/")
setwd(working.dir)
# For google sheet ----
metadata<-gs_title("Paste title of labsheet here")%>%
gs_read_csv(ws = "paste sheet name here")%>%
ga.clean.names()
# For csv file ----
metadata <-ga.list.files("_Metadata.csv")%>% # list all files ending in "_Metadata.csv"
purrr::map_df(~ga.read.files_em.csv(.))%>% # combine into dataframe
dplyr::select(campaignid,sample,latitude,longitude,date,time,location,status,site,depth,observer,successful.count,successful.length,comment)%>% # This line ONLY keep the 15 columns listed. Remove or turn this line off to keep all columns (Turn off with a # at the front).
glimpse()
glimpse()
setwd(staging.dir)
write.csv(metadata,paste(study,"metadata.csv",sep="_"),row.names = FALSE)
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn()%>%
dplyr::inner_join(metadata)%>%
dplyr::filter(successful.count=="Yes")%>%
dplyr::filter(maxn>0)
glimpse()
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn()
rlang::last_error()
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn(1)
## Combine Points and Count files into maxn ----
maxn<-ga.create.maxn(maxn)
ga.create.maxn()
